{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentence generation with a given word -- testing different models for English and Polish"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986820a3a60fe64d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some pre-requisites and things I had to install, mostly written here so I don't forget and that they're all in one place"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2813d5b3d5abbd8f"
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install -U torch -q \n",
    "# #pytorch \n",
    "# #pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \n",
    "# #this version with CUDA required for gemma\n",
    "# !pip install -U transformers datasets -q \n",
    "# !pip install -U accelerate -q \n",
    "# #required to make gemma work"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:40:37.454084Z",
     "start_time": "2024-05-11T17:40:37.450083Z"
    }
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:40:39.796611Z",
     "start_time": "2024-05-11T17:40:37.456084Z"
    }
   },
   "id": "b92603dff9316b45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## English with Gemma 2b (instruct)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dac71d04ef5d430"
  },
  {
   "cell_type": "code",
   "source": [
    "#this model is gated, so... logging in to huggingface with #huggingface-cli login# is necessary\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:42:04.500367Z",
     "start_time": "2024-05-11T17:41:45.282563Z"
    }
   },
   "id": "69846e330ca32e3b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76f9141fadd747859d59b1b1d7553e18"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "input_text = \"Write a sentence that contains the word 'dancing'.\" #...it's the same everytime x_x\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_length=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:42:19.414480Z",
     "start_time": "2024-05-11T17:42:04.502367Z"
    }
   },
   "id": "78eb98d2b398be71",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:573: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a sentence that contains the word 'dancing'.\n",
      "\n",
      "The dancing flames illuminated the night sky, casting a mesmerizing glow on the bustling city streets.<eos>\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample output: (aka the only output this model will give, no randomness here)\n",
    "\n",
    "<bos>Write a sentence that contains the word 'dancing'.\n",
    "\n",
    "The dancing flames illuminated the night sky, casting a mesmerizing glow on the bustling city streets.<eos>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21a5be9ad0696cc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T17:42:28.057059Z",
     "start_time": "2024-05-11T17:42:28.054058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# restart Colab env\n",
    "exit()"
   ],
   "id": "6782d24c136c07d2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "## English with Mistral 7B Instruct v0.2",
   "metadata": {
    "collapsed": false
   },
   "id": "7da9e2948c75128c"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=\"auto\")\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:43:27.446559Z",
     "start_time": "2024-05-11T17:42:31.816138Z"
    }
   },
   "id": "d68120b31b692ddf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\accelerate\\utils\\modeling.py:1363: UserWarning: Current model requires 905976576 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "314dc6a063c5481ca75523ce90f27d05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "human_prompt = \"Write me a sentence containing the word 'dancing'.\"\n",
    "prompt = f\"<s>[INST]{human_prompt}[/INST]\"\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:45:52.542295Z",
     "start_time": "2024-05-11T17:43:27.449561Z"
    }
   },
   "id": "6763b7a58ddd21d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Write me a sentence containing the word 'dancing'.[/INST] Under the twinkling stars, we all joined hands and danced together, lost in the joyful rhythm of the music.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Samples:\n",
    "\n",
    "*(this one took 14 minutes to generate x_x Completely unacceptable...)*\n",
    "\n",
    "[INST]Write me a sentence containing the word 'dancing'.[/INST] \n",
    "- Under the twinkling stars, a group of friends laughed and danced together, creating unforgettable memories at the outdoor summer party.\n",
    "\n",
    "*(this time it took 2.5 min for some reason, which is not great but fine I guess... except...)*\n",
    "\n",
    "- Under the twinkling stars, a group of friends let go of their worries and danced joyfully to the rhythm of the lively music.\n",
    "\n",
    "*(ok what is up with these twinkling stars? x_x)*\n",
    "\n",
    "- Under the twinkling stars, a group of friends laughed and danced together, creating a joyful and unforgettable moment.\n",
    "\n",
    "*(changed temperature from 0.1 to 0.5, let's see if it's any different)*\n",
    "\n",
    "- Under the twinkling fairy lights, a group of friends laughed and cheered as they danced together, lost in the joyful rhythm of the music.\n",
    "\n",
    "*(HOTTER! temperature 0.9!!)*\n",
    "\n",
    "- Amidst the lively tunes of the marimba band, everyone was dancing joyfully in the moonlit garden.\n",
    "\n",
    "*(changed nothing and we're back to the twinkling stars)*\n",
    "\n",
    "- Under the twinkling stars, a group of children laughed and danced together, their joyful movements illuminating the night.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22fdcbd6f7f4951d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## English with quantized Mistral 7B Instruct v0.2 GGUF\n",
    "\n",
    "(https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)\n",
    "\n",
    "Current MVP for English, as it's the fastest and the results are good enough"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cde35eaf9b28c7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip3 install huggingface-hub\n",
    "# !pip install llama-cpp-python\n",
    "# !huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4d87c3c174d4a0e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "    model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "    #chat_format= \"llama-2\", # The chat format to use - either \"llama-1\" or \"llama-2\" -- CAN BE OMITTED FOR DEFAULT FLAVOR\n",
    "    n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "    n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "    n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:49:35.340628Z",
     "start_time": "2024-05-11T17:49:33.565227Z"
    }
   },
   "id": "174ed9c6e90a7c41",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  2144.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"Write me a sentence containing the word 'dances'.\"\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "    f\"<s>[INST] {prompt} [/INST]\", # Prompt\n",
    "    max_tokens=120,  # Generate up to 120 tokens\n",
    "    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "    echo=True        # Whether to echo the prompt\n",
    ")\n",
    "#print(output)\n",
    "print(output[\"choices\"][0][\"text\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:50:10.687662Z",
     "start_time": "2024-05-11T17:49:45.128755Z"
    }
   },
   "id": "9241ac5ec10b7ba8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   20721.89 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    23 runs   (    0.21 ms per token,  4660.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20721.67 ms /    20 tokens ( 1036.08 ms per token,     0.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4640.75 ms /    22 runs   (  210.94 ms per token,     4.74 tokens per second)\n",
      "llama_print_timings:       total time =   25514.43 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Write me a sentence containing the word 'dances'. [/INST] At the annual village festival, children and adults alike gather to dance together, celebrating their rich cultural heritage.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Samples:\n",
    "\n",
    "[INST]Write me a sentence containing the word 'dancing'.\n",
    "\n",
    "- Amidst the lively melody, people of all ages were dancing joyfully together at the outdoor festival.\n",
    "- Amidst the lively tunes of the mariachi band, everyone was dancing and laughing, their joyful spirits intertwined in the warm summer evening.\n",
    "- Under the twinkling stars, they danced together in perfect harmony, their laughter echoing through the night. *(NOOOOOOOO T-T)*\n",
    "- As the sun began to set, the couple lost themselves in the rhythm of the music and started dancing passionately under the twinkling stars. *(IT'S BACK, AT THE BACK)*\n",
    "- As the sun began to set, the crowd at the festival came alive with excitement as they formed a circle and started dancing together, lost in the rhythm of the lively music playing from the speakers.\n",
    "- As the music played, everyone got up from their seats and started dancing enthusiastically, lost in the joy of the moment.\n",
    "- Under the twinkling stars, they all gathered around the bonfire, laughing and clapping as they watched the graceful ballerinas perform enchanting dances at the village festival.\n",
    "\n",
    "*(ok so these are very similar except they take 5 seconds or less, which is obviously strictly better, we'll go with this one after all)*\n",
    "\n",
    "*(also if we change the word at all, we get different results, so that's good too. Something to keep in mind for sure.)*\n",
    "\n",
    "\n",
    "[INST] Write me an example sentence containing the word 'dance'. [/INST] \n",
    "\n",
    "- On the vibrant and colorful stage, the ballerinas danced gracefully to the enchanting melodies of Tchaikovsky's Swan Lake.\n",
    "- Sarah twirled gracefully across the floor as she danced to the lively rhythm of the salsa music at the local community center.\n",
    "\n",
    "\n",
    "[INST] Write me an example sentence containing the word 'dances'. [/INST] \n",
    "- At the annual village festival, children of all ages danced joyfully around the maypole, weaving colorful ribbons in a lively and synchronous pattern.\n",
    "- The colorful performers gracefully danced to the lively rhythm of the traditional music, mesmerizing the crowd with their elegant movements and synchronized steps.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e21fd9295e26179"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Polish with Bielik 7B Instruct v0.1 GPTQ (quantized)\n",
    "\n",
    "Current MVP for Polish, it's fast and does exactly what I need"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dfd868bcf20def8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install optimum -q\n",
    "!pip install auto-gptq -q\n",
    "!pip install ctransformers[cuda]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "139d1c272e4c5593",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\") # good for demo, not good idea for production :)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:59:21.883141Z",
     "start_time": "2024-05-11T17:59:17.705328Z"
    }
   },
   "id": "e7444fba2f2d23e5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"speakleash/Bielik-7B-Instruct-v0.1-GPTQ\" #https://colab.research.google.com/drive/1Al9glPVCuOXbtDsks8cMcuzkuu8YDzpg?usp=sharing#scrollTo=LDo7PEL-gyyy\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')\n",
    "gptq=True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T17:59:40.328663Z",
     "start_time": "2024-05-11T17:59:21.885141Z"
    }
   },
   "id": "2e64a6f379c50106",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\transformers\\modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "human_prompt = \"Napisz zdanie, w którym znajduje się słowo 'taniec'\"\n",
    "prompt = f\"<s>[INST]{human_prompt}[/INST]\"\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1, #0.1 is the default, changing it to .5 leads to... German xD\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T18:04:55.968070Z",
     "start_time": "2024-05-11T18:04:30.072035Z"
    }
   },
   "id": "4307c8192d823c24",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Napisz zdanie, w którym znajduje się słowo 'taniec'[/INST]  Taniec to jedna z najstarszych form sztuki, który łączy w sobie elementy muzyki, ruchu i wyrażania emocji.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Samples:\n",
    "\n",
    "[INST]Proszę napisać mi zdanie zawierające słowo 'taniec'[/INST]  \n",
    "- Taniec to jedna z najstarszych form sztuki, która wyraża emocje, historie i kulturę narodów na całym świecie.\n",
    "\n",
    "\n",
    "[INST]Napisz przykładowe zdanie, w którym się pojawia słowo 'taniec'[/INST]  \n",
    "- Taniec to jedna z najstarszych form sztuki, która wyraża emocje, energię i kreatywność poprzez ruch i muzykę.\n",
    "\n",
    "\n",
    "[INST]Napisz przykładowe zdanie, w którym się pojawia słowo **'tańca'**[/INST]  \n",
    "- Taniec to nie tylko forma rozrywki, ale również sposób na wyrażenie emocji i kreatywności.\n",
    "- Taniec to nie tylko forma rozrywki, ale również sposób na wyrażenie emocji i kreatywności. *(hmm, usually not THIS repetitive)*\n",
    "\n",
    "\n",
    "[INST]Napisz zdanie, w którym się pojawia słowo 'tańce'[/INST]  *(temp 0.3)*\n",
    "- Taniec to jedna z najstarszych form sztuki, która łączy w sobie elementy muzyki, ruchu i wyrażania emocji.\n",
    "\n",
    "\n",
    "[INST]... 'tańce'[/INST]  *(temp 0.4)*\n",
    "- Taniec to jedna z najstarszych form sztuki, która pozwala wyrazić emocje i wyzwolić energię.\n",
    "\n",
    "\n",
    "[INST]... 'tańce'[/INST]  *(temp 0.1)*\n",
    "- Taniec to jedna z najstarszych form sztuki, która wyraża emocje, energię i kulturę narodów całego świata.\n",
    "\n",
    "\n",
    "[INST]Napisz zdanie, używając słowo 'taniec'[/INST]  \n",
    "- Taniec jest jednym z najstarszych i najbardziej uniwersalnych sposobów wyrażania siebie i komunikacji międzyludzkiej.\n",
    "\n",
    "\n",
    "[INST]Napisz zdanie, w którym ktoś robi 'taniec'[/INST]  \n",
    "- W sobotni wieczór na parkiecie królował nie tylko taniec, ale i radość życia. #wayyy too vague of an answer\n",
    "\n",
    "\n",
    "[INST]Napisz zdanie, w którym znajduje się słowo 'taniec'[/INST]  \n",
    "- Taniec to jedna z najstarszych form sztuki, która wyraża emocje, opowiada historie i łączy ludzi.\n",
    "\n",
    "\n",
    "*(temperature 0.5 xDDDDD)*\n",
    "- Fußwork, der die Körpertatsen entfaltt, ist ein Element der Körpertatsen, der auf der Bühne sehr oft genutzt wird.\n",
    "\n",
    "**Aside from the german one, these are pretty samesy, but honestly really functional. I can try to think of other formats for the prompt that could change the output and continue to test those in the future**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c15a6f9985e1176c"
  },
  {
   "cell_type": "code",
   "source": [
    "human_prompt = (\"Napisz zdanie, w którym pojawia się słowo 'tańczący'.\") \n",
    "prompt = f\"<s>[INST]{human_prompt}[/INST]\"\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T18:01:20.245775Z",
     "start_time": "2024-05-11T18:00:55.117690Z"
    }
   },
   "id": "cc30c7df7072fc6c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Napisz zdanie, w którym pojawia się słowo 'tańczący'.[/INST]  Wśród tańczących na parkiecie, tylko ona poruszała się z taką gracją i lekkością.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Samples:\n",
    "\n",
    "[INST]Napisz zdanie zawierające słowo 'tańczący'.[/INST]  \n",
    "- W parku można było zobaczyć grupę tańczących par, które spędzały miło czas na świeżym powietrzu.\n",
    "- W letnie wieczory parki w całym mieście wypełniają się dźwiękami muzyki i radosnymi ludźmi, którzy tańczą, ciesząc się chwilą.\n",
    "- Wśród tańczących na parkiecie, tylko ona poruszała się z taką gracją i lekkością.\n",
    "\n",
    "[INST]Napisz zdanie, w którym pojawia się słowo 'tańczący'.[/INST]\n",
    "- Wśród tańczących na parkiecie, tylko ona poruszała się z taką gracją i lekkością.\n",
    "\n",
    "**Ok, so some words just produce more varied results regardless, which kinda makes sense actually. Maybe other prompt sentences aren't really necessary, but we'll see how it all comes together.**\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bde031a125cd34e"
  },
  {
   "cell_type": "code",
   "source": [
    "human_prompt = (\"Napisz 4 zdania, w ktorych się pojawia słowo 'taniec'.\")\n",
    "prompt = f\"<s>[INST]{human_prompt}[/INST]\"\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T18:03:01.537012Z",
     "start_time": "2024-05-11T18:01:20.246775Z"
    }
   },
   "id": "814ea07c8ee49e61",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Napisz 4 zdania, w ktorych się pojawia słowo 'taniec'.[/INST]  1. Taniec to wyjątkowa forma ekspresji, która łączy w sobie ruch, muzykę i emocje.\n",
      "2. Nauka tańca może być nie tylko przyjemna, ale też bardzo korzystna dla zdrowia.\n",
      "3. Różne style tańca pozwalają każdemu znaleźć coś, co będzie odpowiadało jego osobowości i preferencjom.\n",
      "4. Taniec może być sposobem na wyrażenie siebie, budowanie pewności siebie i rozwijanie umiejętności komunikacji.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Samples:\n",
    "\n",
    "[INST]Napisz 4 przykładowe zdania, w ktorych znajduje się słowo 'taniec'.[/INST]  Oto przykłady zdań, w których pojawia się słowo \"taniec\":\n",
    "\n",
    "1. Taniec to wyrafinowana forma sztuki, która łączy w sobie ruch, muzykę i emocje.\n",
    "2. Julia pokochała taniec od pierwszego wejrzenia, gdy tylko zobaczyła transmisję z baletu.\n",
    "3. W weekendowym parku ludzie tańczyli wokół ognia, świętując nadejście wiosny.\n",
    "4. Taniec towarzyski stał się jego pasją, odkąd zaczął brać lekcje i wygrywać turnieje.\n",
    "\n",
    "[INST]Napisz 4 przykładowe zdania, w ktorych się pojawia słowo 'taniec'.[/INST]  Oto przykładowe zdania, w których pojawia się słowo \"taniec\":\n",
    "\n",
    "1. Taniec to jedna z najstarszych form sztuki, wyrażająca radość i wolność poprzez ruch.\n",
    "2. Uwielbiam patrzeć, jak moje dzieci tańczą bez tremy, wyrażając siebie w sposób naturalny.\n",
    "3. Taniec towarzyski to forma sztuki, która łączy partnerów w harmonijnym ruchu i muzyce.\n",
    "4. Taniec brzucha to technika, która łączy w sobie elementy akrobatyki, izraelskiego folkloru i jogi, wyrażając wewnętrzne piękno kobiecości.\n",
    "\n",
    "...\n",
    "1. Taniec to wyrafinowana forma sztuki, która łączy w sobie ruch, muzykę i emocje.\n",
    "2. Julia kocha taniec i postanowiła zapisać się do szkoły baletowej.\n",
    "3. W trakcie wesela, goście zostali zachęceni do wspólnego tańca i zabawy.\n",
    "4. Taniec towarzyski to forma tańca społecznego, która jest wykonywana w parach do różnych rodzajów muzyki."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ca7c1e2b1dd62ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

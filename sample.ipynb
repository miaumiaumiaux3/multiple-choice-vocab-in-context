{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Some pre-requisites and things I had to install, mostly written here so I don't forget and that they're all in one place"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2813d5b3d5abbd8f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T17:11:27.737910Z",
     "start_time": "2024-04-09T17:11:27.732909Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U torch -q #pytorch \n",
    "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 #this version with CUDA required for gemma\n",
    "!pip install -U transformers datasets -q #huggingface transformers and datasets\n",
    "!pip install -U accelerate -q #required to make gemma work"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T17:40:52.927761Z",
     "start_time": "2024-04-09T17:40:50.952315Z"
    }
   },
   "id": "b92603dff9316b45",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate a sentence containing a given word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986820a3a60fe64d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### English with gemma 2b (instruct)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dac71d04ef5d430"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9982027aa4614b589bf5e87fe3f927e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this model is gated, so... logging in to huggingface with #huggingface-cli login# is necessary\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T20:33:15.842934Z",
     "start_time": "2024-04-13T20:33:05.883652Z"
    }
   },
   "id": "69846e330ca32e3b",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Please write me a sentence that contains the word 'dancing'.\n",
      "\n",
      "The dancing flames illuminated the night sky, casting a mesmerizing glow on the bustling city streets.<eos>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write a sentence that contains the word 'dancing'.\" #...it's the same everytime x_x\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_length=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:51:00.099666Z",
     "start_time": "2024-04-13T19:50:46.295434Z"
    }
   },
   "id": "78eb98d2b398be71",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### English with Mistral 7B Instruct v0.2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7da9e2948c75128c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\accelerate\\utils\\modeling.py:1363: UserWarning: Current model requires 905976576 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2f23fd0f40d4a08878e2fa36e6d1cc7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=\"auto\")\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T21:10:00.553165Z",
     "start_time": "2024-04-13T21:08:54.407802Z"
    }
   },
   "id": "d68120b31b692ddf",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Write me a sentence containing the word 'dancing'.[/INST] Under the twinkling stars, a group of children laughed and danced together, their joyful movements illuminating the night.\n"
     ]
    }
   ],
   "source": [
    "human_prompt = \"Write me a sentence containing the word 'dancing'.\"\n",
    "prompt = f\"<s>[INST]{human_prompt}[/INST]\"\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T21:34:55.807179Z",
     "start_time": "2024-04-13T21:32:46.299781Z"
    }
   },
   "id": "6763b7a58ddd21d9",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Samples:\n",
    "\n",
    "#this one took 14 minutes to generate x_x Completely unacceptable...\n",
    "[INST]Write me a sentence containing the word 'dancing'.[/INST] Under the twinkling stars, a group of friends laughed and danced together, creating unforgettable memories at the outdoor summer party.\n",
    "\n",
    "#this time it took 2.5 min for some reason, which is not great but fine I guess... except...\n",
    "Under the twinkling stars, a group of friends let go of their worries and danced joyfully to the rhythm of the lively music.\n",
    "\n",
    "#ok what is up with these twinkling stars? x_x\n",
    "Under the twinkling stars, a group of friends laughed and danced together, creating a joyful and unforgettable moment.\n",
    "\n",
    "#changed temperature from 0.1 to 0.5, let's see if it's any different\n",
    "Under the twinkling fairy lights, a group of friends laughed and cheered as they danced together, lost in the joyful rhythm of the music.\n",
    "#xDDDDDD\n",
    "\n",
    "#HOTTER! temperature 0.9!!\n",
    "Amidst the lively tunes of the marimba band, everyone was dancing joyfully in the moonlit garden.\n",
    "\n",
    "#changed nothing and we're back to the twinkling stars xD\n",
    "Under the twinkling stars, a group of children laughed and danced together, their joyful movements illuminating the night.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22fdcbd6f7f4951d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### English with quantized Mistral 7B Instruct v0.2 (https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cde35eaf9b28c7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (0.22.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\chase\\appdata\\roaming\\python\\python38\\site-packages (from huggingface-hub) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from huggingface-hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from huggingface-hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from huggingface-hub) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from huggingface-hub) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from requests->huggingface-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from requests->huggingface-hub) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from requests->huggingface-hub) (2024.2.2)\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (0.2.61)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from llama-cpp-python) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from llama-cpp-python) (1.24.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
      ".\\mistral-7b-instruct-v0.2.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf to C:\\Users\\chase\\.cache\\huggingface\\hub\\tmpvlgzelvr\n"
     ]
    }
   ],
   "source": [
    "!pip3 install huggingface-hub\n",
    "!pip install llama-cpp-python\n",
    "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T21:40:07.501964Z",
     "start_time": "2024-04-13T21:39:13.866795Z"
    }
   },
   "id": "f4d87c3c174d4a0e",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  2144.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "    model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "    #chat_format= \"llama-2\", # The chat format to use - either \"llama-1\" or \"llama-2\" -- CAN BE OMITTED FOR DEFAULT FLAVOR\n",
    "    n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "    n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "    n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T21:52:34.740020Z",
     "start_time": "2024-04-13T21:52:28.787670Z"
    }
   },
   "id": "174ed9c6e90a7c41",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1745.18 ms\n",
      "llama_print_timings:      sample time =       4.79 ms /    33 runs   (    0.15 ms per token,  6886.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1034.07 ms /    13 tokens (   79.54 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:        eval time =    5527.43 ms /    32 runs   (  172.73 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =    6652.09 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Write me a sentence containing the word 'dances'. [/INST] The colorful performers gracefully danced to the lively rhythm of the traditional music, mesmerizing the crowd with their elegant movements and synchronized steps.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write me a sentence containing the word 'dances'.\"\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "    f\"<s>[INST] {prompt} [/INST]\", # Prompt\n",
    "    max_tokens=120,  # Generate up to 120 tokens\n",
    "    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "    echo=True        # Whether to echo the prompt\n",
    ")\n",
    "#print(output)\n",
    "print(output[\"choices\"][0][\"text\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T21:58:14.136970Z",
     "start_time": "2024-04-13T21:58:07.475461Z"
    }
   },
   "id": "9241ac5ec10b7ba8",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "Samples:\n",
    "\n",
    "[INST]Write me a sentence containing the word 'dancing'.\n",
    "Amidst the lively melody, people of all ages were dancing joyfully together at the outdoor festival.\n",
    "Amidst the lively tunes of the mariachi band, everyone was dancing and laughing, their joyful spirits intertwined in the warm summer evening.\n",
    "Under the twinkling stars, they danced together in perfect harmony, their laughter echoing through the night. #NOOOOOOOO T-T\n",
    "As the sun began to set, the couple lost themselves in the rhythm of the music and started dancing passionately under the twinkling stars. #IT'S BACK, AT THE BACK\n",
    "As the sun began to set, the crowd at the festival came alive with excitement as they formed a circle and started dancing together, lost in the rhythm of the lively music playing from the speakers.\n",
    "As the music played, everyone got up from their seats and started dancing enthusiastically, lost in the joy of the moment.\n",
    "Under the twinkling stars, they all gathered around the bonfire, laughing and clapping as they watched the graceful ballerinas perform enchanting dances at the village festival.\n",
    "\n",
    "#ok so these are very similar except they take 5 seconds or less, which is obviously strictly better, we'll go with this one after all\n",
    "\n",
    "#also if we change the word at all, we get different results, so that's good too. Something to keep in mind for sure.\n",
    "[INST] Write me an example sentence containing the word 'dance'. [/INST] \n",
    "On the vibrant and colorful stage, the ballerinas danced gracefully to the enchanting melodies of Tchaikovsky's Swan Lake.\n",
    "Sarah twirled gracefully across the floor as she danced to the lively rhythm of the salsa music at the local community center.\n",
    "[INST] Write me an example sentence containing the word 'dances'. [/INST] At the annual village festival, children of all ages danced joyfully around the maypole, weaving colorful ribbons in a lively and synchronous pattern.\n",
    "The colorful performers gracefully danced to the lively rhythm of the traditional music, mesmerizing the crowd with their elegant movements and synchronized steps.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e21fd9295e26179"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Polish with Bielik 7B Instruct v0.1 GPTQ (quantized)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dfd868bcf20def8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install optimum -q\n",
    "!pip install auto-gptq -q\n",
    "!pip install ctransformers[cuda]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:52:57.076059Z",
     "start_time": "2024-04-13T19:52:16.201486Z"
    }
   },
   "id": "139d1c272e4c5593",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\") # good for demo, not good idea for production :)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:53:26.221884Z",
     "start_time": "2024-04-13T19:53:26.217883Z"
    }
   },
   "id": "e7444fba2f2d23e5",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22d3299061ed4b968de442344018bcdf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chase\\.cache\\huggingface\\hub\\models--speakleash--Bielik-7B-Instruct-v0.1-GPTQ. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\transformers\\modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "579fb95beff4453d9d4b7fd115253b78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"speakleash/Bielik-7B-Instruct-v0.1-GPTQ\" #https://colab.research.google.com/drive/1Al9glPVCuOXbtDsks8cMcuzkuu8YDzpg?usp=sharing#scrollTo=LDo7PEL-gyyy\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')\n",
    "gptq=True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:58:08.548295Z",
     "start_time": "2024-04-13T19:54:14.010157Z"
    }
   },
   "id": "2e64a6f379c50106",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Proszę napisać mi zdanie zawierające słowo 'taniec'[/INST]  Taniec to wyjątkowa forma ekspresji, która łączy w sobie ruch, muzykę i emocje.\n"
     ]
    }
   ],
   "source": [
    "human_prompt = \"Proszę napisać mi zdanie zawierające słowo 'taniec'\" #albo tańczący\n",
    "prompt = f\"<s>[INST]{human_prompt}[/INST]\"\n",
    "#print(prompt)\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:59:21.009847Z",
     "start_time": "2024-04-13T19:59:01.821398Z"
    }
   },
   "id": "4307c8192d823c24",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Napisz zdanie zawierające słowo 'tańczący'.[/INST]  W parku można było zobaczyć grupę tańczących par, które spędzały miło czas na świeżym powietrzu.\n"
     ]
    }
   ],
   "source": [
    "human_prompt = (\"Napisz zdanie zawierające słowo 'tańczący'.\") \n",
    "prompt = f\"<s>[INST]{human_prompt}[/INST]\"\n",
    "#print(prompt)\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T20:05:59.716613Z",
     "start_time": "2024-04-13T20:05:36.278178Z"
    }
   },
   "id": "cc30c7df7072fc6c",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\logging\\__init__.py\", line 1085, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\logging\\__init__.py\", line 929, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\logging\\__init__.py\", line 668, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\logging\\__init__.py\", line 373, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\asyncio\\windows_events.py\", line 316, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\asyncio\\events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\chase\\AppData\\Local\\Temp\\ipykernel_27368\\1429790555.py\", line 5, in <module>\n",
      "    outputs = pipe(\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\transformers\\pipelines\\text_generation.py\", line 240, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"C:\\Users\\chase\\anaconda3\\envs\\untitled\\lib\\site-packages\\transformers\\utils\\logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Napisz 4 przykładowe zdania, w ktorych się pojawia słowo 'taniec'.[/INST]  Oto przykładowe zdania, w których pojawia się słowo \"taniec\":\n",
      "\n",
      "1. Taniec to jedna z najstarszych form sztuki, wyrażająca radość i wolność poprzez ruch.\n",
      "2. Uwielbiam patrzeć, jak moje dzieci tańczą bez tremy, wyrażając siebie w sposób naturalny.\n",
      "3. Taniec towarzyski to forma sztuki, która łączy partnerów w harmonijnym ruchu i muzyce.\n",
      "4. Taniec brzucha to technika, która łączy w sobie elementy akrobatyki, izraelskiego folkloru i jogi, wyrażając wewnętrzne piękno kobiecości.\n"
     ]
    }
   ],
   "source": [
    "human_prompt = (\"Napisz 4 przykładowe zdania, w ktorych się pojawia słowo 'taniec'.\")\n",
    "prompt = f\"<s>[INST]{human_prompt}[/INST]\"\n",
    "#print(prompt)\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T20:16:56.513011Z",
     "start_time": "2024-04-13T20:15:07.777630Z"
    }
   },
   "id": "814ea07c8ee49e61",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Samples:\n",
    "\n",
    "[INST]Napisz 4 przykładowe zdania, w ktorych znajduje się słowo 'taniec'.[/INST]  Oto przykłady zdań, w których pojawia się słowo \"taniec\":\n",
    "\n",
    "1. Taniec to wyrafinowana forma sztuki, która łączy w sobie ruch, muzykę i emocje.\n",
    "2. Julia pokochała taniec od pierwszego wejrzenia, gdy tylko zobaczyła transmisję z baletu.\n",
    "3. W weekendowym parku ludzie tańczyli wokół ognia, świętując nadejście wiosny.\n",
    "4. Taniec towarzyski stał się jego pasją, odkąd zaczął brać lekcje i wygrywać turnieje.\n",
    "\n",
    "[INST]Napisz 4 przykładowe zdania, w ktorych się pojawia słowo 'taniec'.[/INST]  Oto przykładowe zdania, w których pojawia się słowo \"taniec\":\n",
    "\n",
    "1. Taniec to jedna z najstarszych form sztuki, wyrażająca radość i wolność poprzez ruch.\n",
    "2. Uwielbiam patrzeć, jak moje dzieci tańczą bez tremy, wyrażając siebie w sposób naturalny.\n",
    "3. Taniec towarzyski to forma sztuki, która łączy partnerów w harmonijnym ruchu i muzyce.\n",
    "4. Taniec brzucha to technika, która łączy w sobie elementy akrobatyki, izraelskiego folkloru i jogi, wyrażając wewnętrzne piękno kobiecości."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ca7c1e2b1dd62ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5965b2bfbe1c1917"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
